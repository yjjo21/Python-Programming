{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8613c45c",
   "metadata": {},
   "source": [
    "# Code Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4cabe",
   "metadata": {},
   "source": [
    "최근에 작성했던 코드를 첨부했습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from datetime import datetime        \n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings(action='ignore')\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ArchiTools():\n",
    "    def __init__(self, sDirHome, sMarket, nAhead, sModelName, nVerbose=1):\n",
    "        self.dirHome = sDirHome\n",
    "        self.market = sMarket\n",
    "        self.ahead = nAhead\n",
    "        self.modelName = sModelName\n",
    "        self.verbose = nVerbose\n",
    "\n",
    "        # Hyper-parameter\n",
    "        self.archiDF = pd.read_csv(self.dirHome+'/psc/model_archi.csv')\n",
    "        archiIdx = self.archiDF[self.archiDF['sModelName'] == self.modelName ].index[0]\n",
    "        self.selection = self.archiDF.loc[archiIdx, 'sSelection']\n",
    "        self.totFeature = self.archiDF.loc[archiIdx, 'nTotFeature']\n",
    "        self.features = self.archiDF.loc[archiIdx, 'nFeatures']\n",
    "\n",
    "        self.timeUnit = self.archiDF.loc[archiIdx, 'nTimeUnit']\n",
    "        self.timeSteps = self.archiDF.loc[archiIdx, 'nTimeSteps']\n",
    "        self.initialState = self.archiDF.loc[archiIdx, 'bInitialState']\n",
    "        self.stacks = self.archiDF.loc[archiIdx, 'nStacks']\n",
    "\n",
    "        self.hiddenNodes = self.archiDF.loc[archiIdx, 'ltHiddenNodes']\n",
    "        self.hiddenNodes = np.int64(self.hiddenNodes.split('_'))\n",
    "        self.regularFactor = self.archiDF.loc[archiIdx, 'nRegularFactor']\n",
    "        self.lossName = self.archiDF.loc[archiIdx, 'sLossName']\n",
    "        self.optimizer = self.archiDF.loc[archiIdx, 'sOptimizer']\n",
    "        self.predIntervalSteps = self.archiDF.loc[archiIdx, 'sOptimizer']\n",
    "\n",
    "        # directory\n",
    "        self.rawDFDir = self.dirHome + \"/out/totalDataMart\"\n",
    "        self.convertedDFDir = self.dirHome + \"/out/convertedData\"\n",
    "        self.varImpDFDir = self.dirHome + \"/out/variableImportance\"\n",
    "        self.trainInfoDir = self.dirHome + '/out/train_info/'\n",
    "\n",
    "        #\n",
    "        self.yName = self.market + '___px_last'\n",
    "        self.yVar = \"{}___px_last_{}\".format(self.market, self.ahead)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "class ReadTools(ArchiTools):\n",
    "    ### Description ###\n",
    "    # Read Data and Seperate Variables by functions\n",
    "    def __init__(self, sDirHome, sMarket, nAhead, sModelName, nVerbose=1):\n",
    "        super().__init__(sDirHome, sMarket, nAhead, sModelName, nVerbose)\n",
    "    \n",
    "    def readRawData(self, bX=False): #1.1\n",
    "        if self.verbose: print(\"({}) [ Read Raw Data ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        if bX: self.rawXDF = pd.read_csv(self.rawDFDir + \"/variables.csv\")\n",
    "        self.rawDF = pd.read_csv(self.rawDFDir + \"/markets.csv\", usecols=['date', self.yName])\n",
    "    \n",
    "    def readConvertedData(self): #1.2\n",
    "        if self.verbose: print(\"({}) [ Read Converted Data ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        tmpFileName = \"/{:03}.csv\".format(self.ahead)\n",
    "        tmpDir = self.convertedDFDir + tmpFileName      \n",
    "        self.convertedDF = pd.read_csv(tmpDir)\n",
    "        self.convertedDF.sort_values(by=['date'], axis=0, ascending=False, inplace=True)\n",
    "        self.convertedDF.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    def readVarImpData(self): #1.3\n",
    "        if self.verbose: print(\"({}) [ Read Variables Importance Data ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        tmpFileName = '/{}/{:03}.csv'.format(self.market, self.ahead)\n",
    "        tmpDir = self.varImpDFDir + tmpFileName\n",
    "        self.varImpDF = pd.read_csv(tmpDir) \n",
    "        self.varImpDF = self.varImpDF[['var_name', 'correlation','impurity', 'SSE', 'matching_score', 'non_zero_number', 'rank_total']]\n",
    "        self.varImpDF['score_1'] = (abs(self.varImpDF[['correlation']])-abs(self.varImpDF[['correlation']]).mean())/abs(self.varImpDF[['correlation']]).std()\n",
    "        self.varImpDF['score_2'] = (self.varImpDF[['impurity']]-self.varImpDF[['impurity']].mean())/self.varImpDF[['impurity']].std()\n",
    "        self.varImpDF['score_3'] = -(self.varImpDF[['SSE']]-self.varImpDF[['SSE']].mean())/(self.varImpDF[['SSE']].std())\n",
    "        self.varImpDF['score_4'] = -(self.varImpDF[['matching_score']]-self.varImpDF[['matching_score']].mean())/self.varImpDF[['matching_score']].std()\n",
    "        self.varImpDF['score_5'] = (self.varImpDF[['non_zero_number']]-self.varImpDF[['non_zero_number']].mean())/self.varImpDF[['non_zero_number']].std()\n",
    "        \n",
    "        if self.selection == 'SEL01':\n",
    "            self.varImpDF['score'] = self.varImpDF[['score_1','score_2','score_3','score_4','score_5']].mean(1)\n",
    "        elif self.selection == 'SEL02':\n",
    "            self.varImpDF['score'] = self.varImpDF['score_3']\n",
    "        elif self.selection == 'SEL03':\n",
    "            self.varImpDF['score'] = self.varImpDF['score_5'] * 0.8 + self.varImpDF['score_2'] *0.2\n",
    "        \n",
    "        self.varImpDF.sort_values('score', ascending=False, inplace=True)\n",
    "        self.varImpDF = self.varImpDF.iloc[:self.totFeature,]\n",
    "        self.varImpDF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def slicingData(self): #1.4\n",
    "            self.xVars = self.convertedDF.columns[1:80].tolist()\n",
    "            self.convertedDF = self.convertedDF[['date'] + [self.yVar] + self.xVars]\n",
    "\n",
    "        \n",
    "    def slicingData0(self): #1.4\n",
    "        if self.varChecker()==1:\n",
    "            self.xVars = self.varImpDF['var_name'].tolist()\n",
    "            self.xVars = list(set(self.xVars).intersection(self.convertedDF.columns.tolist())) #############################################################################3###### tmp code\n",
    "            self.convertedDF = self.convertedDF[['date'] + [self.yVar] + self.xVars]\n",
    "        \n",
    "    def naChecker(self): #1.5\n",
    "        cntNA = []\n",
    "        for i in range(self.convertedDF.shape[1]):\n",
    "            cntNA.append(pd.isna(self.convertedDF[self.convertedDF.columns[i]]).sum())\n",
    "        idxNACols = np.where(np.array(cntNA)>0)[0]\n",
    "        NACols = self.convertedDF.columns[idxNACols]\n",
    "        rangeNA = []\n",
    "        for i in idxNACols:\n",
    "            idxNARows = np.where(pd.isna(self.convertedDF[self.convertedDF.columns[int(i)]]))[0]\n",
    "            tmpNADF = self.convertedDF['date'][idxNARows]\n",
    "            rangeNA.append(min(tmpNADF) + '~'  + max(tmpNADF))\n",
    "        return pd.DataFrame({'market':self.market,\n",
    "                             'ahead': self.ahead,\n",
    "                             'na_column':NACols,'range':rangeNA})\n",
    "    \n",
    "    def varChecker(self): #1.6\n",
    "        tmpNum = len(set(self.convertedDF.columns).intersection(set(self.varImpDF['var_name'])))\n",
    "        if tmpNum==self.varImpDF.shape[0]:\n",
    "            return 1\n",
    "        else:\n",
    "            print('Variables are not enough to be applied to the GRU.\\n')\n",
    "            return 0\n",
    "    \n",
    "    def naRemover(self): #1.7\n",
    "        if self.verbose: print(\"({}) [ Remove Non-Available Data ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        tmp = self.convertedDF.dropna()\n",
    "        tmp.reset_index(drop=True, inplace=True)\n",
    "        print(\"{:23} * {} rows are removed!\".format('',self.convertedDF.shape[0] - tmp.shape[0]))\n",
    "        self.convertedDF = tmp\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class DataTools(ReadTools):\n",
    "    def __init__(self, sDirHome, sMarket, nAhead, sModelName, sTrainEnd, nVerbose=1):\n",
    "        super().__init__(sDirHome, sMarket, nAhead, sModelName, nVerbose)\n",
    "        # Date-parameters\n",
    "        self.trainEnd = sTrainEnd\n",
    "        self.trainStart = pd.date_range(self.trainEnd, periods=2, freq='-519B').strftime('%Y-%m-%d')[1]\n",
    "        self.validEnd = pd.date_range(self.trainStart, periods=2, freq='-1B').strftime('%Y-%m-%d')[1]\n",
    "        self.validStart = pd.date_range(self.validEnd, periods=2, freq='-64B').strftime('%Y-%m-%d')[1]\n",
    "        self.minmaxStart = pd.date_range(self.trainStart, periods=2, freq='-{}B'.\\\n",
    "                                        format(self.timeSteps*self.timeUnit)).strftime('%Y-%m-%d')[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def dateMaker(self): #2.1\n",
    "        self.convertedDF.rename(columns={'date':'date_base'}, inplace=True)\n",
    "        def internalDateGen(date):\n",
    "            return pd.date_range(date, periods=2, freq='{}B'.format(self.ahead)).strftime('%Y-%m-%d').tolist()[1]\n",
    "        self.convertedDF['date_frct'] = self.convertedDF['date_base'].apply(lambda x: internalDateGen(x))\n",
    "        self.convertedDF = self.convertedDF[['date_base', 'date_frct'] + [self.yVar] + self.xVars]\n",
    "\n",
    "    def minmaxComputer(self): #2.2\n",
    "        tmp = self.convertedDF.loc[(self.convertedDF['date_frct']>=self.minmaxStart)]\n",
    "        tmp = tmp.loc[(tmp['date_frct']<=self.trainEnd)][self.xVars]\n",
    "        listMin = tmp.min().tolist()\n",
    "        listMax = tmp.max().tolist()\n",
    "        self.minmaxDF = pd.DataFrame({'var_name': self.xVars, \n",
    "                                      'range':'{} ~ {}'.format(self.minmaxStart, self.trainEnd),\n",
    "                                      'min':listMin, \n",
    "                                      'max': listMax})\n",
    "        self.minmaxDF['use_ox'] = np.where(self.minmaxDF['max']-self.minmaxDF['min']!=0, 1, 0)\n",
    "        self.xVarsUse = self.minmaxDF['var_name'].loc[self.minmaxDF['use_ox']==1].tolist()\n",
    "\n",
    "    def minmaxNormalization(self): #2.3\n",
    "        def internalMinMax(series):\n",
    "            tmpMin = self.minmaxDF.loc[self.minmaxDF['var_name']==series.name]['min'].tolist()[0]\n",
    "            tmpMax = self.minmaxDF.loc[self.minmaxDF['var_name']==series.name]['max'].tolist()[0]\n",
    "            return (series-tmpMin)/(tmpMax-tmpMin)\n",
    "        \n",
    "        self.normalizedDF = pd.concat([self.convertedDF[['date_base','date_frct']+ [self.yVar]],\n",
    "                                       self.convertedDF[self.xVarsUse].apply(lambda x: internalMinMax(x))], axis=1)\n",
    "\n",
    "    def toNumpy(self, nUnitInterval, nRepeatedCells, bNormal=False, bYOn=False): #2.4\n",
    "        print(\"({}) [ Data Preprocessing ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        print('{:23} - Reshape Data (converting data frame to numpy.)'.format(''))\n",
    "        if bNormal:\n",
    "            self.normalizedDF.sort_values(by=['date_frct'], ascending=False, inplace=True)\n",
    "            dateDF = self.normalizedDF[['date_base', 'date_frct']]\n",
    "            yDF = self.normalizedDF[[self.yVar]]\n",
    "            xDF = self.normalizedDF[self.xVarsUse]\n",
    "        else:\n",
    "            self.convertedDF.sort_values(by=['date_frct'], ascending=False, inplace=True)\n",
    "            dateDF = self.convertedDF[['date_base', 'date_frct']]\n",
    "            yDF = self.convertedDF[[self.yVar]]\n",
    "            xDF = self.convertedDF[self.xVars]\n",
    "        # batch index\n",
    "        dimBatch = len(xDF) - nUnitInterval*(nRepeatedCells-1)\n",
    "        listIdx = [np.arange(x, nUnitInterval*nRepeatedCells + x, nUnitInterval) for x in range(dimBatch)]\n",
    "        # to numpy\n",
    "        def internal1(data, index):\n",
    "            return np.array(data.iloc[list(reversed(index)), ])\n",
    "        def internal2(data, index):\n",
    "            return data.iloc[index, ].max()\n",
    "        flagDateDF = pd.DataFrame(map(internal2, repeat(dateDF), listIdx))\n",
    "        xNP = np.array(list(map(internal1, repeat(xDF), listIdx)))\n",
    "        if bYOn: \n",
    "            yNP = np.array(list(map(internal1, repeat(yDF),listIdx)))\n",
    "            return flagDateDF, xNP, yNP\n",
    "        else:\n",
    "            return flagDateDF, xNP\n",
    "\n",
    "\n",
    "\n",
    "class ModelTools():\n",
    "    def __init__(self, nTimeSteps, nFeatures, bInitialState, nStacks, ltHiddenNodes, nRegularFactor, sLossName, sOptimizer):\n",
    "        self.nTimeSteps = nTimeSteps\n",
    "        self.nFeatures = nFeatures\n",
    "        self.bInitialState = bInitialState\n",
    "        self.nStacks = nStacks\n",
    "        self.ltHiddenNodes = ltHiddenNodes\n",
    "        self.nRegularFactor = nRegularFactor\n",
    "        self.sLossName = sLossName\n",
    "        self.sOptimizer = sOptimizer\n",
    "        self.modelgen()\n",
    "        self.modelcompile()\n",
    "\n",
    "    def modelgen(self):\n",
    "        layer_input = keras.Input(shape=(self.nTimeSteps, self.nFeatures), name='input')\n",
    "\n",
    "        # Extract Initial State?\n",
    "        if self.bInitialState:\n",
    "            layer_state = keras.layers.GRU(units=self.ltHiddenNodes[0], return_state=True, name='gru_initial_state')(layer_input)[1]\n",
    "            layer_gru = keras.layers.GRU(units=self.ltHiddenNodes[0], return_sequences=True, name='gru_cell_0')(layer_input, initial_state=layer_state)\n",
    "        else:\n",
    "            layer_gru = keras.layers.GRU(units=self.ltHiddenNodes[0], return_sequences=True, name='gru_cell_0')(layer_input)\n",
    "\n",
    "        # Create GRU Cells if two more cell were needed.\n",
    "        if self.nStacks>1:\n",
    "            for x in range(self.nStacks-1):\n",
    "                layer_gru = keras.layers.GRU(units=self.ltHiddenNodes[x+1], return_sequences=True, name='gru_cell_{}'.format(x+1))(layer_gru)\n",
    "\n",
    "        # Output layer\n",
    "        layer_output = keras.layers.TimeDistributed(keras.layers.Dense(units=1, activation='linear',\n",
    "            kernel_regularizer=keras.regularizers.l2(l=self.nRegularFactor),\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(seed=925)), name='output')(layer_gru)\n",
    "\n",
    "        # Deploy Model\n",
    "        self.gru_model = keras.Model(layer_input, layer_output)\n",
    "\n",
    "    def modelcompile(self):\n",
    "        # Loss\n",
    "        if self.sLossName == 'LOSS01':\n",
    "            def customLoss(y_true, y_pred):\n",
    "                mean_true = tf.math.reduce_mean(y_true)\n",
    "                mean_pred = tf.math.reduce_mean(y_pred)\n",
    "\n",
    "                meanerr = tf.math.sqrt(tf.math.square(mean_true - mean_pred))\n",
    "                rmserr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_true-y_pred)))\n",
    "                signerr = 1.0-tf.math.reduce_mean(tf.math.multiply(tf.math.sign(y_true), tf.math.sign(y_pred)))\n",
    "\n",
    "                return rmserr + meanerr + signerr\n",
    "        if self.sLossName == 'LOSS02':\n",
    "            def customLoss(y_true, y_pred):\n",
    "                mean_true = tf.math.reduce_mean(y_true)\n",
    "                mean_pred = tf.math.reduce_mean(y_pred)\n",
    "\n",
    "                std_true = tf.math.reduce_std(y_true)\n",
    "                std_pred = tf.math.reduce_std(y_pred)\n",
    "\n",
    "                up = tf.reduce_mean(tf.math.multiply((y_true-mean_true), (y_pred-mean_pred)))\n",
    "                down = std_true * std_pred\n",
    "                corr = 1.0-tf.compat.v1.where(tf.math.is_nan(up/down),0.0,up/down)\n",
    "\n",
    "                rmserr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_true-y_pred)))\n",
    "                signerr = 1-tf.math.reduce_mean(tf.math.multiply(tf.math.sign(y_true), tf.math.sign(y_pred)))\n",
    "\n",
    "                return rmserr + corr/2 + signerr/2\n",
    "        if self.sLossName == 'LOSS03':\n",
    "            def customLoss(y_true, y_pred):\n",
    "                cumsum_true = tf.math.cumsum(y_true)\n",
    "                cumsum_pred = tf.math.cumsum(y_pred)\n",
    "\n",
    "                cumsum_mean_true = tf.math.reduce_mean(cumsum_true)\n",
    "                cumsum_mean_pred = tf.math.reduce_mean(cumsum_pred)\n",
    "\n",
    "                mean_true = tf.math.reduce_mean(y_true)\n",
    "                mean_pred = tf.math.reduce_mean(y_pred)\n",
    "\n",
    "                std_true = tf.math.reduce_std(y_true)\n",
    "                std_pred = tf.math.reduce_std(y_pred)\n",
    "\n",
    "                up = tf.reduce_mean(tf.math.multiply((y_true-mean_true), (y_pred-mean_pred)))\n",
    "                down = std_true * std_pred\n",
    "                corr = 1.0-tf.compat.v1.where(tf.math.is_nan(up/down),0.0,up/down)\n",
    "\n",
    "                cumsumerr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(cumsum_true-cumsum_pred)))\n",
    "                cummeanerr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(cumsum_mean_true-cumsum_mean_pred)))\n",
    "\n",
    "                rmserr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_true-y_pred)))\n",
    "                meanerr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(mean_true-mean_pred)))\n",
    "                signerr = 1-tf.math.reduce_mean((tf.math.multiply(tf.math.sign(y_true), tf.math.sign(y_pred))+1.0)/2.0)\n",
    "\n",
    "                return (cumsumerr+cummeanerr)*0.3+(rmserr+signerr+corr/2.0)*0.7\n",
    "        if self.sLossName == 'LOSS04':\n",
    "            def customLoss(y_true, y_pred):\n",
    "                cumsum_true = tf.math.cumsum(y_true)\n",
    "                cumsum_pred = tf.math.cumsum(y_pred)\n",
    "\n",
    "                cumsum_mean_true = tf.math.reduce_mean(cumsum_true)\n",
    "                cumsum_mean_pred = tf.math.reduce_mean(cumsum_pred)\n",
    "\n",
    "                mean_true = tf.math.reduce_mean(y_true)\n",
    "                mean_pred = tf.math.reduce_mean(y_pred)\n",
    "\n",
    "                std_true = tf.math.reduce_std(y_true)\n",
    "                std_pred = tf.math.reduce_std(y_pred)\n",
    "\n",
    "                up = tf.reduce_mean(tf.math.multiply((y_true-mean_true), (y_pred-mean_pred)))\n",
    "                down = std_true * std_pred\n",
    "                corr = 1.0-tf.compat.v1.where(tf.math.is_nan(up/down),0.0,up/down)\n",
    "\n",
    "                cumsumerr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(cumsum_true-cumsum_pred)))\n",
    "                cummeanerr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(cumsum_mean_true-cumsum_mean_pred)))\n",
    "\n",
    "                rmserr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_true-y_pred)))\n",
    "                meanerr = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(mean_true-mean_pred)))\n",
    "                signerr = 1-tf.math.reduce_mean((tf.math.multiply(tf.math.sign(y_true), tf.math.sign(y_pred))+1.0)/2.0)\n",
    "\n",
    "                return (cumsumerr+cummeanerr)*0.2+(rmserr+signerr+corr/2.0)*0.8\n",
    "        if self.sLossName == 'LOSS05':\n",
    "            def customLoss(y_true, y_pred):\n",
    "                mean_true = tf.math.reduce_mean(y_true)\n",
    "                mean_pred = tf.math.reduce_mean(y_pred)\n",
    "                mserr = tf.math.reduce_mean(tf.math.square(y_true-y_pred))\n",
    "                return mserr\n",
    "\n",
    "        # Optimizer\n",
    "        if self.sOptimizer == 'Adam':\n",
    "            self.gru_model.compile(loss=customLoss, optimizer=keras.optimizers.Adam())\n",
    "        if self.sOptimizer == 'RMSprop':\n",
    "            self.gru_model.compile(loss=customLoss, optimizer=keras.optimizers.RMSprop())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# 1. Get Data.\n",
    "# 2. Train GRU Model.\n",
    "# 3. Save model weights.\n",
    "########################################\n",
    "\n",
    "\n",
    "class TrainTools(DataTools):\n",
    "    def __init__(self, sDirHome, sMarket, nAhead, sModelName, sTrainEnd, nVerbose=1):\n",
    "        super().__init__(sDirHome, sMarket, nAhead, sModelName, sTrainEnd, nVerbose)\n",
    "\n",
    "        # # process\n",
    "        # self.getData()\n",
    "        # self.preprocData()\n",
    "        # self.splitData()\n",
    "        # self.training()\n",
    "        # self.saveTrainInfo()\n",
    "\n",
    "    def getData(self):\n",
    "        self.readConvertedData()\n",
    "        self.readVarImpData()\n",
    "        self.readRawData()\n",
    "        self.slicingData()\n",
    "        self.naRemover()\n",
    "\n",
    "    def preprocData(self):\t\n",
    "        self.dateMaker()\n",
    "        self.minmaxComputer()\n",
    "        self.minmaxNormalization()\n",
    "        self.dateDF, self.xNP, self.yNP = self.toNumpy(self.timeUnit, self.timeSteps, bNormal=True, bYOn=True)\n",
    "        print(self.dateDF.shape, self.xNP.shape, self.yNP.shape)\n",
    "\n",
    "\n",
    "    def splitData(self):\n",
    "        self.trainYNP = self.yNP[(self.dateDF['date_frct']>=self.trainStart)&(self.dateDF['date_frct']<=self.trainEnd),:,:]\n",
    "        self.validXNP = self.xNP[(self.dateDF['date_frct']>=self.validStart)&(self.dateDF['date_frct']<=self.validEnd),:,:]\n",
    "        self.trainXNP = self.xNP[(self.dateDF['date_frct']>=self.trainStart)&(self.dateDF['date_frct']<=self.trainEnd),:,:]\n",
    "        self.validYNP = self.yNP[(self.dateDF['date_frct']>=self.validStart)&(self.dateDF['date_frct']<=self.validEnd),:,:]\n",
    "\n",
    "    def training(self, nEpochs=200):\n",
    "        print(\"({}) [ Train GRU Models ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        # select colunms\n",
    "        self.colNumLT = [np.random.choice(self.totFeature, self.features, replace=False) for x in range(10)]\n",
    "        # deploy 10 models\n",
    "        def modelgen():\n",
    "            return ModelTools(self.timeSteps,\n",
    "                              self.features,\n",
    "                              self.initialState,\n",
    "                              self.stacks,\n",
    "                              self.hiddenNodes,\n",
    "                              self.regularFactor,\n",
    "                              self.lossName,\n",
    "                              self.optimizer)\n",
    "\n",
    "        self.MODELTOOLS = [modelgen() for x in range(10)]\n",
    "        earlyStop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=30)\n",
    "\n",
    "        self.trErr = []\n",
    "        for x in range(10):\n",
    "            print('{:23}   : Model {:02}'.format('', x))\n",
    "            self.MODELTOOLS[x].gru_model.fit(self.trainXNP[:,:,self.colNumLT[x]], self.trainYNP,\n",
    "                                             validation_data = (self.validXNP[:,:,self.colNumLT[x]], self.validYNP),\n",
    "                                             batch_size=20,\n",
    "                                             epochs=nEpochs,\n",
    "                                             callbacks=[earlyStop],\n",
    "                                             verbose=0)\n",
    "            while np.isnan(self.MODELTOOLS[x].gru_model.predict(self.trainXNP[:,:,self.colNumLT[x]])[:,-1,-1]).sum()!=0:\n",
    "                self.MODELTOOLS[x] = modelgen()\n",
    "                self.colNumLTT[x] = np.random.choice(self.totFeature, self.features, replace=False)\n",
    "                self.MODELTOOLS[x].gru_model.fit(self.trainXNP[:,:,self.colNumLT[x]], self.trainYNP,\n",
    "                                             validation_data = (self.validXNP[:,:,self.colNumLT[x]], self.validYNP),\n",
    "                                             batch_size=20,\n",
    "                                             epochs=nEpochs,\n",
    "                                             callbacks=[earlyStop],\n",
    "                                             verbose=0)\n",
    "            self.trErr.append(((self.MODELTOOLS[x].gru_model.predict(self.trainXNP[:,:,self.colNumLT[x]])[:,-1,-1]-self.trainYNP[:,-1,-1])**2).sum())\n",
    "\n",
    "        self.avgWeights = (1-np.array(self.trErr))/(1-np.array(self.trErr)).sum()\n",
    "\n",
    "        print('{:23}  - complete!'.format(''))\n",
    "\n",
    "\n",
    "    def saveTrainInfo(self):\n",
    "        print(\"({}) [ Save Training Information ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        trainInfoDir = self.trainInfoDir + '/out/train_info/{}/{}'.format(self.trainEnd, self.modelName)\n",
    "\n",
    "        modelWeightsDir = trainInfoDir + '/model_weights/{}/{:03}'.format(self.market, self.ahead)\n",
    "        avgWeightsDir = trainInfoDir + '/avg_weights/{}/{:03}'.format(self.market, self.ahead)\n",
    "        colNumDir = trainInfoDir + '/columns_number/{}/{:03}'.format(self.market, self.ahead)\n",
    "\n",
    "        if not os.path.isdir(modelWeightsDir):\n",
    "            os.makedirs(modelWeightsDir)\n",
    "\n",
    "        if not os.path.isdir(avgWeightsDir):\n",
    "            os.makedirs(avgWeightsDir)\n",
    "\n",
    "        if not os.path.isdir(colNumDir):\n",
    "            os.makedirs(colNumDir)\n",
    "\n",
    "        for x in range(10):\n",
    "            self.MODELTOOLS[x].gru_model.save_weights(modelWeightsDir+'/model_{:02}.h5'.format(x))\n",
    "            with open(avgWeightsDir+'/model_{:02}.pkl'.format(x), 'wb') as f:\n",
    "                pickle.dump(self.avgWeights[x], f, pickle.HIGHEST_PROTOCOL)\n",
    "            with open(colNumDir+'/model_{:02}.pkl'.format(x), 'wb') as f:\n",
    "                pickle.dump(self.colNumLT[x], f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def trainingN(self, nNumTrain, nEpochs=200):\n",
    "        \"\"\"\n",
    "        부분 컬럼 생성하는 것 제거함. 컬럼이 모두 들어가는 형태로 변경\n",
    "        뒤에 다른 메소드랑 맞지 않을 것임ㄴ\n",
    "        \"\"\"\n",
    "        print(\"({}) [ Train GRU Models ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        # deploy 10 models\n",
    "        def modelgen():\n",
    "            return ModelTools(self.timeSteps,\n",
    "                              self.features,\n",
    "                              self.initialState,\n",
    "                              self.stacks,\n",
    "                              self.hiddenNodes,\n",
    "                              self.regularFactor,\n",
    "                              self.lossName,\n",
    "                              self.optimizer)\n",
    "\n",
    "        self.MODELTOOLS = [modelgen() for x in range(nNumTrain)]\n",
    "        earlyStop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=30)\n",
    "\n",
    "        self.trErr = []\n",
    "        for x in range(nNumTrain):\n",
    "            print('{:23}   : Model {:02}'.format('', x))\n",
    "            self.MODELTOOLS[x].gru_model.fit(self.trainXNP, self.trainYNP,\n",
    "                                             validation_data = (self.validXNP, self.validYNP),\n",
    "                                             batch_size=20,\n",
    "                                             epochs=nEpochs,\n",
    "                                             callbacks=[earlyStop],\n",
    "                                             verbose=0)\n",
    "            while np.isnan(self.MODELTOOLS[x].gru_model.predict(self.trainXNP)[:,-1,-1]).sum()!=0:\n",
    "                self.MODELTOOLS[x] = modelgen()\n",
    "                self.colNumLTT[x] = np.random.choice(self.totFeature, self.features, replace=False)\n",
    "                self.MODELTOOLS[x].gru_model.fit(self.trainXNP, self.trainYNP,\n",
    "                                             validation_data = (self.validXNP, self.validYNP),\n",
    "                                             batch_size=20,\n",
    "                                             epochs=nEpochs,\n",
    "                                             callbacks=[earlyStop],\n",
    "                                             verbose=0)\n",
    "            self.trErr.append(((self.MODELTOOLS[x].gru_model.predict(self.trainXNP)[:,-1,-1]-self.trainYNP[:,-1,-1])**2).sum())\n",
    "\n",
    "        self.avgWeights = (1-np.array(self.trErr))/(1-np.array(self.trErr)).sum()\n",
    "\n",
    "        print('{:23}  - complete!'.format(''))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predValidation(self):\n",
    "        print(\"({}) [ predValidation ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        for x in range(10):\n",
    "            inputs = np.where(self.validNP[:,:,self.colNumLT[x]]>1, 1, self.validNP[:,:,self.colNumLT[x]])\n",
    "            inputs = np.where(inputs<0, 0, inputs)\n",
    "            if x==0:\n",
    "                prediction0 = self.MODELTOOLS[x].gru_model.predict(inputs)[:,-1,-1] * self.avgWeights[x]\n",
    "                prediction1 = self.MODELTOOLS[x].gru_model.predict(self.xNP[:,:,self.colNumLT[x]])[:,-1,-1] * self.avgWeights[x]\n",
    "            else:\n",
    "                prediction0 += self.MODELTOOLS[x].gru_model.predict(inputs)[:,-1,-1] * self.avgWeights[x]\n",
    "                prediction1 += self.MODELTOOLS[x].gru_model.predict(self.xNP[:,:,self.colNumLT[x]])[:,-1,-1] * self.avgWeights[x]\n",
    "\n",
    "        self.prediction = prediction0*0.8 + prediction1*0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ForecastTools(DataTools):\n",
    "    def __init__(self, sDirHome, sMarket, nAhead, sModelName, sTrainEnd, sTestStart, sTestEnd, nVerbose=1):\n",
    "        super().__init__(sDirHome, sMarket, nAhead, sModelName, sTrainEnd, nVerbose)\n",
    "        print('Setup')\n",
    "        # Generate Models\n",
    "        self.MODELTOOLS = [ModelTools(self.timeSteps,\n",
    "                                      self.features,\n",
    "                                      self.initialState,\n",
    "                                      self.stacks,\n",
    "                                      self.hiddenNodes,\n",
    "                                      self.regularFactor,\n",
    "                                      self.lossName,\n",
    "                                      self.optimizer) for x in range(10)]\n",
    "\n",
    "        # Date Parameters\n",
    "        self.testStart = sTestStart\n",
    "        self.testEnd = sTestEnd\n",
    "\n",
    "        # # Process\n",
    "        # self.getData()\n",
    "        # self.preprocData()\n",
    "        # self.readTrainInfo()\n",
    "        # self.forecasting()\n",
    "        # self.makeOutput()\n",
    "        # self.saveForecastInfo()\n",
    "\n",
    "    def getTestData(self):\n",
    "        self.readConvertedData(self.testEnd)\n",
    "        self.readVarImpData(self.totFeature, self.trainEnd)\n",
    "        self.readRawData(self.testEnd)\n",
    "        self.slicingData()\n",
    "\n",
    "    def preprocTestData(self):\t\n",
    "        self.minmaxComputer()\n",
    "        self.minmaxNormalization()\n",
    "        dateDF, xNP = self.toNumpy(self.timeUnit, self.timeSteps, bNormal=True, bYOn=False)\n",
    "        # Target data\n",
    "        self.dateDF = dateDF.iloc[0:1]\n",
    "        self.xNP = xNP[0:1, :, :]\n",
    "\n",
    "\n",
    "\n",
    "    def readTrainInfo(self):\n",
    "        print(\"({}) [ Read Training Information ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        trainInfoDir = self.trainInfoDir + '/out/train_info/{}/{}'.format(self.trainEnd, self.modelName)\n",
    "\n",
    "        modelWeightsDir = trainInfoDir + '/model_weights/{}/{:03}'.format(self.market, self.ahead)\n",
    "        avgWeightsDir = trainInfoDir + '/avg_weights/{}/{:03}'.format(self.market, self.ahead)\n",
    "        colNumDir = trainInfoDir + '/columns_number/{}/{:03}'.format(self.markets, self.ahead)\n",
    "\n",
    "        self.avgWeights = []\n",
    "        self.colNumLT = []\n",
    "        for x in range(10):\n",
    "            self.MODELTOOLS[x].gru_model.load_weights(modelWeightsDir+'/model_{:02}.h5'.format(x))\n",
    "            with open(avgWeightsDir+'/model_{:02}.pkl'.format(x), 'rb') as f:\n",
    "                self.avgWeights.append(pickle.load(f))\n",
    "            with open(colNumDir+'/model_{:02}.pkl'.format(x), 'rb') as f:\n",
    "                self.colNumLT.append(pickle.load(f))\n",
    "\n",
    "\n",
    "\n",
    "    def forecasting(self):\n",
    "        print(\"({}) [ Forecasting '{}' {} Ahead ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), self.market, self.ahead))\n",
    "        for x in range(10):\n",
    "            inputs = np.where(self.xNP[:,:,self.colNumLT[x]]>1, 1, self.xNP[:,:,self.colNumLT[x]])\n",
    "            inputs = np.where(inputs<0, 0, inputs)\n",
    "            if x==0:\n",
    "                prediction0 = self.MODELTOOLS[x].gru_model.predict(inputs)[:,-1,-1] * self.avgWeights[x]\n",
    "                prediction1 = self.MODELTOOLS[x].gru_model.predict(self.xNP[:,:,self.colNumLT[x]])[:,-1,-1] * self.avgWeights[x]\n",
    "            else:\n",
    "                prediction0 += self.MODELTOOLS[x].gru_model.predict(inputs)[:,-1,-1] * self.avgWeights[x]\n",
    "                prediction1 += self.MODELTOOLS[x].gru_model.predict(self.xNP[:,:,self.colNumLT[x]])[:,-1,-1] * self.avgWeights[x]\n",
    "\n",
    "        self.prediction = prediction0*0.8 + prediction1*0.2\n",
    "\n",
    "    def makeOutput(self):\n",
    "        stdDF = self.convertedDF[['date']+self.yVar]\n",
    "        stdDF.sort_values('date', ascending=False, inplace=True)\n",
    "        stdDF.dropna(inplace=True)\n",
    "        stdDF.reset_index(drop=True, inplace=True)\n",
    "        stdDF = stdDF.iloc[:self.predIntervalSteps, 1]\n",
    "        movingStd = stdDF.std()\n",
    "\n",
    "        self.outputDF = pd.DataFrame({'BSE_DT':[self.dateDF.iloc[0,0]],\n",
    "                                      'CST_TGT_PD_CD':[self.market],\n",
    "                                      'CST_MD_GB_CD':['A1'],\n",
    "                                      'CST_DT_GB_CD':[self.ahead],\n",
    "                                      'CST_DT':[self.dateDF.iloc[0,1]],\n",
    "                                      'IDX_BSE_VAL':[self.rawDF.iloc[-1,1]],\n",
    "                                      'CST_IDX_BSE_VAL':[self.rawDF.iloc[-1,1]*(1+self.prediction[0])],\n",
    "                                      'CST_IDX_MIN_VAL':[self.rawDF.iloc[-1,1]*(1+self.prediction[0]-movingStd*1.65)],\n",
    "                                      'CST_IDX_MAX_VAL':[self.rawDF.iloc[-1,1]*(1+self.prediction[0]+movingStd*1.65)],\n",
    "                                      'CST_IDX_BSE_RT':[self.prediction[0]],\n",
    "                                      'CST_IDX_MIN_RT':[self.prediction[0]-movingStd*1.65],\n",
    "                                      'CST_IDX_MAX_RT':[self.prediction[0]+movingStd*1.65]})\n",
    "\n",
    "\n",
    "    def saveForecastInfo(self):\n",
    "        print(\"({}) [ Save Forecast Information ]\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        frctInfoDir = self.homeDir + '/out/forecast_info/{}'.format(self.sWorkingDate)\n",
    "        outputDir = frctInfoDir + '/output'\n",
    "\n",
    "        if not os.path.isdir(outputDir):\n",
    "            os.makedirs(outputDir)\n",
    "\n",
    "        self.outputDF.to_csv(outputDir+'/{}_{:03}.csv'.format(self.market, self.ahead), index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
